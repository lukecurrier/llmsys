{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threads\n",
    "CPUs have 8-16 threads currently on most laptops\n",
    "GPUs have thousands!\n",
    "Catch: threads on a GPU are in lockstep, meaning they all have to be doing the same task.\n",
    "\n",
    "Let's say you have a branch - a CPU can run true and false branches simultaneously, but a GPU has to do true branches, then false branches (or vice versa).\n",
    "\n",
    "Moral of the story is that you need both - CPU for general purpose, GPU for raw speed on certain tasks\n",
    "\n",
    "Don't forget to move devices correctly!\n",
    "\n",
    "### Transformers\n",
    "All of these models are transformers\n",
    "\n",
    "Technically for lec2 we're working with decoder-only transformers\n",
    "\n",
    "Three parts to a transformer: \n",
    "1. The Embedding Layer\n",
    "\n",
    "Embedding layer transforms numbers into very high-dimensional vectors (3000 levels), representing in some way the 'meaning' of the words. It understands words in these higher dimensions based on its training. \n",
    "\n",
    "Embeddings that are close to each other in value are often close to each other in meaning - there's something interpretable here. For some kinds of models you can take \"king\", subtract the value for \"man\", add the value for \"woman\", and arrive at something similar to \"queen\".\n",
    "\n",
    "The embedding layer is pretty simple - it's basically a dictionary. It maps tokens to those high-dim vectors.\n",
    "\n",
    "2. The Transformer\n",
    "\n",
    "Transformers have some number of layers (Llama has 28/32). They take a vector and produce a vector of the same size, but with additional information added to it. \n",
    "\n",
    "Ocean -> noun -> blue -> water -> deeper and deeper info\n",
    "\n",
    "It ALSO gets information not just from the embedding above it, but from all previous embeddings of a level higher. So, as soon as you're past that first embedding, you gain a ton of context and can start inferring more words. \n",
    "\n",
    "It keeps doing this until we reach a level of information such that we can actually predict the next word. Crazy stuff!\n",
    "\n",
    "We're at the point in the science where we can actually really dig into the internals of these transformers.\n",
    "\n",
    "3. The Unembedding Layer\n",
    "\n",
    "Takes the transformed embeddings and gives you a probability distribution over all possible words such that higher-likelihood tokens are reasonable to put next. \n",
    "\n",
    "We also get distributions from each of the previous embeddings as well, though! \n",
    "\n",
    "### Batching\n",
    "\n",
    "What happens when you feed in two prompts of different lengths at once?\n",
    "\n",
    "| Hello | I | am |  ...vs... | Sh | akespeare| was | a | \n",
    "\n",
    "GPUs NEED to operate on the extra space. So, what if we did this?\n",
    "\n",
    "| Hello | I | am | null |\n",
    "\n",
    "Nope, this will produce nonsense! The model has never seen the null token, so it just goes haywire. The solution?\n",
    "\n",
    "| null | Hello | I | am |\n",
    "\n",
    "Pad on the left. \n",
    "\n",
    "<| end of text |> is used to separate documents during training. The model learns that this token represents an informational barrier where no context should flow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
